import json
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm

# On importe tes fonctions depuis main.py
# Assure-toi que main.py est bien dans le m√™me dossier src/
try:
    from main import load_resources, retrieve
except ImportError:
    # Fallback si lanc√© depuis la racine
    import sys
    sys.path.append("src")
    from main import load_resources, retrieve

# Chemins
BASE_DIR = Path(__file__).resolve().parent.parent
DATASET_PATH = BASE_DIR / "data" / "golden_dataset.json"

def calculate_metrics(retrieved_docs, expected_ids):
    """
    V√©rifie si l'un des documents attendus est pr√©sent dans les r√©sultats r√©cup√©r√©s.
    C'est ce qu'on appelle le "Recall@k".
    """
    if not expected_ids:
        return False, 0.0 # Cas des questions pi√®ges (trait√©es √† part)
    
    found_ids = [d['doc_id'] for d in retrieved_docs]
    
    # Est-ce qu'on a trouv√© au moins UN bon document ?
    success = any(e_id in found_ids for e_id in expected_ids)
    
    # Calcul du score moyen des bons documents trouv√©s
    relevant_scores = [d['score'] for d in retrieved_docs if d['doc_id'] in expected_ids]
    avg_score = sum(relevant_scores) / len(relevant_scores) if relevant_scores else 0.0
    
    return success, avg_score

def main():
    print("=== D√©marrage de l'√©valuation automatique ===")
    
    # 1. Chargement du Golden Dataset
    if not DATASET_PATH.exists():
        print(f"[ERREUR] Le fichier {DATASET_PATH} n'existe pas.")
        return
    
    with open(DATASET_PATH, "r", encoding="utf-8") as f:
        dataset = json.load(f)
    print(f"[INFO] {len(dataset)} questions charg√©es depuis le Golden Dataset.")

    # 2. Chargement du moteur RAG
    print("[INFO] Chargement des ressources RAG (Index, Mod√®le...)...")
    df, index, embed_model = load_resources()
    
    # 3. Boucle d'√©valuation
    results = []
    correct_retrieval_count = 0
    trap_success_count = 0
    
    total_scorable = 0 # On ne compte pas les pi√®ges dans le score de retrieval pur
    
    print("\n--- Lancement des tests ---")
    for item in tqdm(dataset):
        query = item["question"]
        q_type = item["type"]
        expected_ids = item.get("expected_doc_ids", [])
        
        # Lancer la recherche (Top 5)
        retrieved_contexts = retrieve(query, df, index, embed_model, top_k=5)
        
        # Analyse des r√©sultats
        if q_type == "trap":
            # Pour un pi√®ge, succ√®s = scores tr√®s bas (tous < 0.5 par exemple)
            max_score = max([c['score'] for c in retrieved_contexts]) if retrieved_contexts else 0
            is_success = max_score < 0.5
            if is_success:
                trap_success_count += 1
            
            results.append({
                "id": item["id"],
                "type": "trap",
                "success": is_success,
                "info": f"Max Score: {max_score:.4f} (Seuil: 0.5)"
            })
            
        else:
            # Pour une question normale
            total_scorable += 1
            is_success, avg_conf = calculate_metrics(retrieved_contexts, expected_ids)
            
            if is_success:
                correct_retrieval_count += 1
            
            # On note quel ID a √©t√© trouv√© pour le d√©bug
            found_ids = [c['doc_id'] for c in retrieved_contexts]
            results.append({
                "id": item["id"],
                "type": q_type,
                "success": is_success,
                "info": f"Attendu: {expected_ids} | Trouv√©: {found_ids}"
            })

    # 4. G√©n√©ration du Rapport
    accuracy = (correct_retrieval_count / total_scorable * 100) if total_scorable > 0 else 0
    
    print("\n" + "="*40)
    print("       RAPPORT D'√âVALUATION RAG       ")
    print("="*40)
    print(f"Questions valides test√©es : {total_scorable}")
    print(f"Questions pi√®ges test√©es  : {len(dataset) - total_scorable}")
    print("-" * 40)
    print(f"‚úÖ PR√âCISION DU RETRIEVAL : {accuracy:.2f}%")
    print("-" * 40)
    
    if len(dataset) - total_scorable > 0:
        trap_acc = (trap_success_count / (len(dataset) - total_scorable)) * 100
        print(f"üõ°Ô∏è  FILTRAGE DES PI√àGES    : {trap_acc:.2f}%")
        print("(Capacit√© √† d√©tecter le hors-sujet via le seuil)")
    
    print("-" * 40)
    print("D√©tails par question :")
    for res in results:
        icon = "‚úÖ" if res["success"] else "‚ùå"
        print(f"{icon} [Q{res['id']} - {res['type']}] {res['info']}")
        
    # Sauvegarde CSV
    pd.DataFrame(results).to_csv(BASE_DIR / "data" / "evaluation_results.csv", index=False)
    print(f"\nRapport d√©taill√© sauvegard√© dans data/evaluation_results.csv")

if __name__ == "__main__":
    main()