import json
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
import sys


# Mettre √† True pour tester le Multi-Query, False pour la recherche simple
USE_MULTI_QUERY = True 
OUTPUT_FILENAME = "evaluation_results_multiquery.csv" if USE_MULTI_QUERY else "evaluation_results_baseline.csv"


# On essaie d'importer les fonctions depuis main.py
try:
    from main import load_resources, retrieve, retrieve_multi_query
except ImportError:
    sys.path.append("src")
    from main import load_resources, retrieve, retrieve_multi_query

# Chemins
BASE_DIR = Path(__file__).resolve().parent.parent
DATASET_PATH = BASE_DIR / "data" / "golden_dataset.json"

def calculate_metrics(retrieved_docs, expected_ids):
    """
    V√©rifie si l'un des documents attendus est pr√©sent dans les r√©sultats r√©cup√©r√©s.
    """
    if not expected_ids:
        return False, 0.0 # Cas des questions pi√®ges
    
    found_ids = [d.get('doc_id') for d in retrieved_docs]
    
    # Est-ce qu'on a trouv√© au moins UN bon document ?
    success = any(e_id in found_ids for e_id in expected_ids)
    
    # Calcul du score moyen des bons documents trouv√©s
    relevant_scores = [d['score'] for d in retrieved_docs if d.get('doc_id') in expected_ids]
    avg_score = sum(relevant_scores) / len(relevant_scores) if relevant_scores else 0.0
    
    return success, avg_score

def main():
    mode_str = "MULTI-QUERY" if USE_MULTI_QUERY else "SIMPLE RETRIEVAL"
    print(f"=== D√©marrage de l'√©valuation automatique : {mode_str} ===")
    
    # Chargement du Golden Dataset
    if not DATASET_PATH.exists():
        print(f"[ERREUR] Le fichier {DATASET_PATH} n'existe pas.")
        return
    
    with open(DATASET_PATH, "r", encoding="utf-8") as f:
        dataset = json.load(f)
    print(f"[INFO] {len(dataset)} questions charg√©es depuis le Golden Dataset.")

    # Chargement du moteur RAG
    print("[INFO] Chargement des ressources RAG (Index, Mod√®le...)...")
    df, index, embed_model = load_resources()
    
    # Boucle d'√©valuation
    results = []
    correct_retrieval_count = 0
    trap_success_count = 0
    
    total_scorable = 0 
    
    print("\n--- Lancement des tests ---")
    for item in tqdm(dataset):
        query = item["question"]
        q_type = item["type"]
        expected_ids = item.get("expected_doc_ids", [])
        
        if USE_MULTI_QUERY:
            # On utilise la fonction avanc√©e
            retrieved_contexts = retrieve_multi_query(query, df, index, embed_model, top_k=5)
        else:
            # On utilise l'ancienne fonction simple
            retrieved_contexts = retrieve(query, df, index, embed_model, top_k=5)
        
        # Analyse des r√©sultats
        if q_type == "trap":
            # Pour un pi√®ge, succ√®s = aucun document trouv√© OU scores tr√®s bas
            if not retrieved_contexts:
                max_score = 0
            else:
                max_score = max([c['score'] for c in retrieved_contexts])
            
            # On consid√®re r√©ussi si le score max est sous le seuil de pertinence (ex: 0.5)
            is_success = max_score < 0.5
            if is_success:
                trap_success_count += 1
            
            results.append({
                "id": item["id"],
                "question": query,
                "type": "trap",
                "success": is_success,
                "info": f"Max Score: {max_score:.4f} (Seuil: 0.5)"
            })
            
        else:
            # Pour une question normale
            total_scorable += 1
            is_success, avg_conf = calculate_metrics(retrieved_contexts, expected_ids)
            
            if is_success:
                correct_retrieval_count += 1
            
            found_ids = [c.get('doc_id') for c in retrieved_contexts]
            results.append({
                "id": item["id"],
                "question": query,
                "type": q_type,
                "success": is_success,
                "info": f"Attendu: {expected_ids} | Trouv√©: {found_ids}"
            })

    # G√©n√©ration du Rapport
    accuracy = (correct_retrieval_count / total_scorable * 100) if total_scorable > 0 else 0
    
    print("\n" + "="*40)
    print(f"    RAPPORT D'√âVALUATION ({mode_str})    ")
    print("="*40)
    print(f"Questions valides test√©es : {total_scorable}")
    print("-" * 40)
    print(f"‚úÖ PR√âCISION DU RETRIEVAL : {accuracy:.2f}%")
    print("-" * 40)
    
    nb_traps = len(dataset) - total_scorable
    if nb_traps > 0:
        trap_acc = (trap_success_count / nb_traps) * 100
        print(f"üõ°Ô∏è  FILTRAGE DES PI√àGES    : {trap_acc:.2f}%")
    
    print("-" * 40)
    print("D√©tails par question :")
    for res in results:
        icon = "‚úÖ" if res["success"] else "‚ùå"
        print(f"{icon} [Q{res['id']}] {res['info']}")
        
    # Sauvegarde CSV
    out_path = BASE_DIR / "data" / OUTPUT_FILENAME
    pd.DataFrame(results).to_csv(out_path, index=False)
    print(f"\nRapport d√©taill√© sauvegard√© dans {out_path}")

if __name__ == "__main__":
    main()